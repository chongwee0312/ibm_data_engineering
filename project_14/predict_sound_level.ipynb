{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Machine Learning Pipeline for Airfoil Noise Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a data engineer at an aeronautics consulting company. Your company prides itself in being able to efficiently design airfoils for use in planes and sports cars. Data scientists in your office need to work with different algorithms and data in different formats. While they are good at Machine Learning, they count on you to be able to do ETL jobs and build ML pipelines. In this project you will use the modified version of the NASA Airfoil Self Noise dataset. You will clean this dataset, by dropping the duplicate rows, and removing the rows with null values. You will create an ML pipe line to create a model that will predict the SoundLevel based on all the other columns. You will evaluate the model and towards the end you will persist the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. Perform ETL activity\n",
    "    - Load the CSV File into A Dataframe\n",
    "    - Duplicates\n",
    "    - Missing Values\n",
    "    - Transformation\n",
    "    - Save the Cleaned Dataset\n",
    "    - Part 1 Evaluation\n",
    "2. Create a  Machine Learning Pipeline\n",
    "    - Load the Dataset from the Saved File\n",
    "    - Define the Pipeline Stages\n",
    "    - Build the pipeline\n",
    "    - Train/Test Split\n",
    "    - Model Training\n",
    "    - Part 2 Evaluation\n",
    "3. Evaluate the Model\n",
    "    - Prediction\n",
    "    - Evaluation of the Model\n",
    "    - Part 3 Evaluation\n",
    "4. Persist the Model\n",
    "    - Save the Model\n",
    "    - Load the Model\n",
    "    - Make predictions with the Loaded Model on the Testdata\n",
    "    - Compare the Predictions from the Loaded Model and the Original Model\n",
    "    - Part 4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    " - The original dataset can be found here NASA airfoil self noise dataset. https://archive.ics.uci.edu/dataset/291/airfoil+self+noise\n",
    " \n",
    " - This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram of an airfoil. - For informational purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Airfoil with flow](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/images/Airfoil_with_flow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram showing the Angle of attack. - For informational purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Airfoil angle of attack](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/images/Airfoil_angle_of_attack.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data manipulation and preprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, count, col, sum\n",
    "import os\n",
    "\n",
    "# Machine learning pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Model evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Model Persistence\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform ETL activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the CSV File into A Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-12 09:19:45--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60682 (59K) [text/csv]\n",
      "Saving to: ‘NASA_airfoil_noise_raw.csv.19’\n",
      "\n",
      "NASA_airfoil_noise_ 100%[===================>]  59.26K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-02-12 09:19:45 (40.8 MB/s) - ‘NASA_airfoil_noise_raw.csv.19’ saved [60682/60682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/12 09:19:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/12 09:19:50 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('NASA').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = spark.read.csv('NASA_airfoil_noise_raw.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevel|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|      800|          0.0|     0.3048|              71.3|             0.00266337|   126.201|\n",
      "|     1000|          0.0|     0.3048|              71.3|             0.00266337|   125.201|\n",
      "|     1250|          0.0|     0.3048|              71.3|             0.00266337|   125.951|\n",
      "|     1600|          0.0|     0.3048|              71.3|             0.00266337|   127.591|\n",
      "|     2000|          0.0|     0.3048|              71.3|             0.00266337|   127.461|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first five rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Frequency: integer (nullable = true)\n",
      " |-- AngleOfAttack: double (nullable = true)\n",
      " |-- ChordLength: double (nullable = true)\n",
      " |-- FreeStreamVelocity: double (nullable = true)\n",
      " |-- SuctionSideDisplacement: double (nullable = true)\n",
      " |-- SoundLevel: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the shcema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the total number of rows\n",
    "rowcount1 = df.count()\n",
    "print(rowcount1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=======================================>               (54 + 10) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevel|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|     1000|         15.6|     0.1016|              39.6|              0.0528487|   119.224|\n",
      "|      800|         15.6|     0.1016|              39.6|              0.0528487|   118.964|\n",
      "|     3150|         15.6|     0.1016|              71.3|              0.0437259|   116.468|\n",
      "|     1600|         15.6|     0.1016|              39.6|              0.0528487|   114.554|\n",
      "|      400|         15.6|     0.1016|              39.6|              0.0528487|   120.484|\n",
      "|     3150|         15.6|     0.1016|              39.6|              0.0528487|   109.254|\n",
      "|     1250|         15.6|     0.1016|              39.6|              0.0528487|   118.214|\n",
      "|     2500|         15.6|     0.1016|              39.6|              0.0528487|   110.264|\n",
      "|     4000|         15.6|     0.1016|              39.6|              0.0528487|   106.604|\n",
      "|      500|         15.6|     0.1016|              39.6|              0.0528487|   115.304|\n",
      "|      200|         15.6|     0.1016|              39.6|              0.0528487|   123.514|\n",
      "|     6300|         15.6|     0.1016|              39.6|              0.0528487|   104.204|\n",
      "|      315|         15.6|     0.1016|              39.6|              0.0528487|   122.754|\n",
      "|     2000|         15.6|     0.1016|              39.6|              0.0528487|   110.894|\n",
      "|     5000|         15.6|     0.1016|              39.6|              0.0528487|   106.224|\n",
      "|     2500|         15.6|     0.1016|              71.3|              0.0437259|   118.998|\n",
      "|      250|         15.6|     0.1016|              39.6|              0.0528487|   124.644|\n",
      "|      630|         15.6|     0.1016|              39.6|              0.0528487|   118.084|\n",
      "|     4000|         15.6|     0.1016|              71.3|              0.0437259|   113.298|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "\n",
      "Total duplicated rows: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates_df = df.exceptAll(df.dropDuplicates())\n",
    "duplicates_count = duplicates_df.count()\n",
    "\n",
    "duplicates_df.show()\n",
    "print('Total duplicated rows:', duplicates_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop all the duplicate rows from the dataset\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:====================================================> (194 + 6) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the number of rows after dropping duplicates\n",
    "rowcount2 = df.count()\n",
    "print(rowcount2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevel|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|      630|          0.0|     0.3048|              null|             0.00310138|   128.629|\n",
      "|     2500|          1.5|     0.3048|              null|             0.00392107|   120.981|\n",
      "|     null|          0.0|     0.3048|              55.5|             0.00283081|   123.236|\n",
      "|      800|          3.0|       null|              39.6|             0.00495741|   129.552|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "\n",
      "Missing values per column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=====================================================>(199 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevel|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|        1|            0|          1|                 2|                      0|         0|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "\n",
      "Total rows with missing values:: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "null_df = df.exceptAll(df.dropna())\n",
    "null_count = null_df.count()\n",
    "null_count_per_column = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "print('Rows with missing values')\n",
    "null_df.show()\n",
    "\n",
    "print('Missing values per column')\n",
    "null_count_per_column.show()\n",
    "\n",
    "print('Total rows with missing values::', null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:=============================================>       (170 + 11) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the number of rows after dropping missing values\n",
    "rowcount3 = df.count()\n",
    "print(rowcount3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename the column 'SoundLevel' to 'SoundLevelDecibels'\n",
    "df = df.withColumnRenamed('SoundLevel', 'SoundLevelDecibels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Cleaned Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reduce the number of partitions in the dataframe to one\n",
    "df = df.repartition(1)\n",
    "\n",
    "# Save the dataframe in parquet format\n",
    "df.write.mode('overwrite').parquet('NASA_airfoil_noise_cleaned.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 - Evaluation\n",
      "\n",
      "Initial number of rows: 1522 \n",
      "\n",
      "Total rows with duplicates: 19\n",
      "Total rows after dropping duplicated rows: 1503 \n",
      "\n",
      "Total rows with missing values: 4\n",
      "Total rows after dropping rows with duplicates and missing values: 1499 \n",
      "\n",
      "New column name: SoundLevelDecibels \n",
      "\n",
      "NASA_airfoil_noise_cleaned.parquet exists: True\n"
     ]
    }
   ],
   "source": [
    "# Part 1 evaluation\n",
    "print('Part 1 - Evaluation\\n')\n",
    "\n",
    "# Initial number of rows\n",
    "print('Initial number of rows:', rowcount1, '\\n')\n",
    "\n",
    "# Total rows after droping duplicates\n",
    "print('Total rows with duplicates:', duplicates_count)\n",
    "print('Total rows after dropping duplicated rows:', rowcount2, '\\n')\n",
    "\n",
    "# Total rows after dropping missing values\n",
    "print('Total rows with missing values:', null_count)\n",
    "print('Total rows after dropping rows with duplicates and missing values:', rowcount3, '\\n')\n",
    "\n",
    "# Renamed column\n",
    "print('New column name:', df.columns[-1], '\\n')\n",
    "\n",
    "# Check if the cleaned dataset is saved\n",
    "print('NASA_airfoil_noise_cleaned.parquet exists:', os.path.isdir('NASA_airfoil_noise_cleaned.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a  Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Dataset from the Saved File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the cleaned data into a dataframe\n",
    "df = spark.read.parquet('NASA_airfoil_noise_cleaned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+------------------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevelDecibels|\n",
      "+---------+-------------+-----------+------------------+-----------------------+------------------+\n",
      "|     4000|          3.0|     0.3048|              31.7|             0.00529514|           115.608|\n",
      "|     3150|          2.0|     0.2286|              31.7|             0.00372371|           121.527|\n",
      "|     2000|          7.3|     0.2286|              31.7|              0.0132672|           115.309|\n",
      "|     2000|          5.4|     0.1524|              71.3|             0.00401199|           131.111|\n",
      "|      500|          9.9|     0.1524|              71.3|              0.0193001|           131.279|\n",
      "+---------+-------------+-----------+------------------+-----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the first five rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n"
     ]
    }
   ],
   "source": [
    "# Find the total number of rows\n",
    "rowcount4 = df.count()\n",
    "print(rowcount4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Pipeline Stages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 1 - Assemble the input columns into a single column 'features'. Use all the columns except SoundLevelDecibels as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the VectorAssembler pipeline stage\n",
    "inputCols = [col for col in df.columns if col != 'SoundLevelDecibels']\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 2 - Scale the 'features' using standard scaler and store in 'scaledFeatures' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Define the StandardScaler pipeline stage\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 3 - Create a LinearRegression model to predict the label 'SoundLevelDecibels'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LinearRegression stage\n",
    "lr = LinearRegression(featuresCol='scaledFeatures', labelCol='SoundLevelDecibels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a pipeline using the above three stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets in a ration of 70:30\n",
    "(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/12 09:21:51 WARN util.Instrumentation: [560aecd1] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/02/12 09:21:51 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/02/12 09:21:51 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/02/12 09:21:51 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "24/02/12 09:21:51 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline with the training data\n",
    "pipelineModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 - Evaluation\n",
      "\n",
      "Total rows: 1499 \n",
      "\n",
      "Pipeline Stage 1: VectorAssembler\n",
      "Pipeline Stage 2: StandardScaler\n",
      "Pipeline Stage 3: LinearRegression \n",
      "\n",
      "Feature columns: ['Frequency', 'AngleOfAttack', 'ChordLength', 'FreeStreamVelocity', 'SuctionSideDisplacement']\n",
      "Label column: SoundLevelDecibels\n"
     ]
    }
   ],
   "source": [
    "# Part 2 evaluation\n",
    "print('Part 2 - Evaluation\\n')\n",
    "\n",
    "# Total rows of the loaded dataset\n",
    "print('Total rows:', rowcount4, '\\n')\n",
    "\n",
    "# Total pipeline stages\n",
    "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
    "print('Pipeline Stage 1:', ps[0])\n",
    "print('Pipeline Stage 2:', ps[1])\n",
    "print('Pipeline Stage 3:', ps[2], '\\n')\n",
    "\n",
    "# Features and label\n",
    "print('Feature columns:', pipelineModel.stages[0].getInputCols())\n",
    "print('Label column:', lr.getLabelCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on testing data\n",
    "predictions = pipelineModel.transform(testingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 21.69767261083989\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='SoundLevelDecibels', metricName='mse')\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print('MSE:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.674707483470427\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='SoundLevelDecibels', metricName='mae')\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared (R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.5382661885816811\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R-squared score\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='SoundLevelDecibels', metricName='r2')\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print('R-squared:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 - Evaluation\n",
      "\n",
      "Number of stages in the pipeline: 3 \n",
      "\n",
      "Coefficient of Frequency is -4.0307\n",
      "Coefficient of AngleOfAttack is -2.1385\n",
      "Coefficient of ChordLength is -3.1762\n",
      "Coefficient of FreeStreamVelocity is 1.6308\n",
      "Coefficient of SuctionSideDisplacement is -2.0552\n",
      "Intercept: 132.23\n",
      "\n",
      "Mean Squared Error: 21.70\n",
      "Mean Absolute Error: 3.67\n",
      "R-squared: 0.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 3 evaluation\n",
    "print('Part 3 - Evaluation\\n')\n",
    "\n",
    "# Total stages\n",
    "totalstages = len(pipelineModel.stages)\n",
    "print('Number of stages in the pipeline:', totalstages, '\\n')\n",
    "\n",
    "# Coefficients and intercept of the model\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "inputcolumns = pipelineModel.stages[0].getInputCols()\n",
    "for col,coefficient in zip(inputcolumns, lrModel.coefficients):\n",
    "    print(f'Coefficient of {col} is {coefficient:.4f}')\n",
    "print(f'Intercept: {lrModel.intercept:.2f}\\n')\n",
    "\n",
    "# Model Evaluation\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'Mean Absolute Error: {mae:.2f}')\n",
    "print(f'R-squared: {r2:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persist the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the pipeline model\n",
    "pipelineModel.write().overwrite().save('final_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_project exists: True\n"
     ]
    }
   ],
   "source": [
    "# Check if the model is saved\n",
    "print('final_project exists:', os.path.isdir('final_project'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the pipeline model\n",
    "loadedPipelineModel = PipelineModel.load('final_project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions with the Loaded Model on the Testdata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the loaded pipeline model and make predictions using testingData\n",
    "loaded_predictions = loadedPipelineModel.transform(testingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the Predictions from the Loaded Model and the Original Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from the loaded model\n",
      "+------------------+------------------+\n",
      "|SoundLevelDecibels|        prediction|\n",
      "+------------------+------------------+\n",
      "|           118.129| 125.1422875397131|\n",
      "|           128.679|122.70503286089041|\n",
      "|           130.989|123.66788978786391|\n",
      "|           134.319|127.56113462661494|\n",
      "|           124.987|120.83603611988016|\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Predictions from the original model\n",
      "+------------------+------------------+\n",
      "|SoundLevelDecibels|        prediction|\n",
      "+------------------+------------------+\n",
      "|           118.129| 125.1422875397131|\n",
      "|           128.679|122.70503286089041|\n",
      "|           130.989|123.66788978786391|\n",
      "|           134.319|127.56113462661494|\n",
      "|           124.987|120.83603611988016|\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 5 rows of the predictions from the loaded model\n",
    "print('Predictions from the loaded model')\n",
    "loaded_predictions.select('SoundLevelDecibels', 'prediction').show(5)\n",
    "\n",
    "# Show the top 5 rows of predictions from the original model\n",
    "print('\\nPredictions from the original model')\n",
    "predictions.select('SoundLevelDecibels', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 4 - Evaluation\n",
      "\n",
      "Number of stages in the pipeline: 3 \n",
      "\n",
      "Coefficient of Frequency is -4.0307\n",
      "Coefficient of AngleOfAttack is -2.1385\n",
      "Coefficient of ChordLength is -3.1762\n",
      "Coefficient of FreeStreamVelocity is 1.6308\n",
      "Coefficient of SuctionSideDisplacement is -2.0552\n",
      "Intercept: 132.23\n"
     ]
    }
   ],
   "source": [
    "# Part 4 evaluation\n",
    "print('Part 4 - Evaluation\\n')\n",
    "\n",
    "# Total stages\n",
    "totalstages = len(loadedPipelineModel.stages)\n",
    "print('Number of stages in the pipeline:', totalstages, '\\n')\n",
    "\n",
    "# Coefficients and intercept of the model\n",
    "loadedmodel = loadedPipelineModel.stages[-1]\n",
    "inputcolumns = loadedPipelineModel.stages[0].getInputCols()\n",
    "for col,coefficient in zip(inputcolumns, loadedmodel.coefficients):\n",
    "    print(f'Coefficient of {col} is {coefficient:.4f}')\n",
    "print(f'Intercept: {loadedmodel.intercept:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop the spark session to free up the memory\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ramesh Sannareddy](https://www.linkedin.com/in/rsannareddy/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork866-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-26|0.1|Ramesh Sannareddy|Initial Version Created|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
